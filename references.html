<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>References | ACIA Framework</title>
    <style>
        body { font-family: 'Inter', sans-serif; margin: 0; padding: 0; background-color: #f7f7f7; color: #333; line-height: 1.6; }
        .page-container { max-width: 900px; margin: 50px auto; padding: 40px; background: white; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); }
        h2 { font-size: 2em; color: #1a73e8; border-bottom: 2px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
        p { margin-bottom: 1.5em; font-size: 1.1em; text-align: justify; }
        .references { list-style: none; padding: 0; }
        .references li { margin-bottom: 1.5em; text-indent: -2em; padding-left: 2em; font-size: 0.95em; }
    </style>
</head>
<body>
    <div class="page-container">
        <p><a href="index.html">← Back to Main Page</a></p>

        <h2>9. References</h2>

        <ol class="references">
            <li>Ahuja, K., Caballero, E., Zhang, D., Gagnon-Audet, J.-C., Bengio, Y., Mitliagkas, I., & Rish, I. (2021). Invariance principle meets information bottleneck for out-of-distribution generalization. <em>Advances in Neural Information Processing Systems</em>.</li>
            <li>Ahuja, K., Mahajan, D., Wang, Y. (2023). Interventional causal representation learning. <em>International conference on machine learning</em>.</li>
            <li>Ahuja, K., Mansouri, A., & Wang, Y. (2024). Multi-domain causal representation learning via weak distributional invariances. <em>Artificial Intelligence and Statistics</em>.</li>
            <li>Arevalo, C. A., Noorbakhsh, S. L., Dong, Y., Hong, Y., & Wang, B. (2024). Task-agnostic privacy-preserving representation learning for federated learning against attribute inference attacks. <em>AAAI Conference on Artificial Intelligence</em>.</li>
            <li>Arjovsky, M., Bottou, L., Gulrajani, I., & Lopez-Paz, D. (2019). Invariant risk minimization. <em>arXiv preprint arXiv:1907.02893</em>.</li>
            <li>Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen, M., Bejnordi, B. E., Lee, B., Paeng, K., Zhong, A., & others. (2018). From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. <em>IEEE Transactions on Medical Imaging</em>.</li>
            <li>Beckers, S., & Halpern, J. Y. (2019). Abstracting causal models. <em>Proceedings of the aaai conference on artificial intelligence</em>, 2678–2685.</li>
            <li>Bentkus, V. (2004). On Hoeffding’s inequalities. <em>The Annals of Probability</em>.</li>
            <li>Bellot, A., & Bareinboim, E. (2024). Partial transportability for domain generalization. <em>The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>.</li>
            <li>Brehmer, J., De Haan, P., Lippe, P., & Cohen, T. S. (2022). Weakly supervised causal representation learning. <em>Advances in Neural Information Processing Systems</em>.</li>
            <li>Buchholz, S., Rajendran, G., Rosenfeld, E., Aragam, B., Schölkopf, B., & Ravikumar, P. (2024). Learning linear causal representations from interventions under general nonlinear mixing. <em>Advances in Neural Information Processing Systems</em>, 36.</li>
            <li>Castro, D. C., Walker, I., & Glocker, B. (2020). Causality matters in medical imaging. <em>Nature Communications</em>, 11(1), 1–10.</li>
            <li>Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y., & Park, S. (2021). Swad: Domain generalization by seeking flat minima. <em>Advances in Neural Information Processing Systems</em>, 34, 22405–22418.</li>
            <li>Chen, Y., Huang, W., Zhou, K., Bian, Y., Han, B., & Cheng, J. (2024). Understanding and Improving Feature Learning for Out-of-Distribution Generalization. <em>Advances in Neural Information Processing Systems</em>, 36.</li>
            <li>Gal, Y., & Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. <em>International conference on machine learning</em>, 1050–1059.</li>
            <li>Gamella, J. L., & Heinze-Deml, C. (2020). Active invariant causal prediction: Experiment selection through stability. <em>Advances in Neural Information Processing Systems</em>.</li>
            <li>Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., March, M., & Lempitsky, V. (2016). Domain-adversarial training of neural networks. <em>Journal of machine learning research</em>, 17(59), 1–35.</li>
            <li>Geiger, A., Lu, H., Icard, T., & Potts, C. (2021). Causal abstractions of neural networks. <em>Advances in Neural Information Processing Systems</em>, 34, 9574–9586.</li>
            <li>Gui, S., Liu, M., Li, X., Luo, Y., & Ji, S. (2024). Joint learning of label and environment causal independence for graph out-of-distribution generalization. <em>Advances in Neural Information Processing Systems</em>, 36.</li>
            <li>Gulrajani, I., & Lopez-Paz, D. (2020). In search of lost domain generalization. <em>arXiv preprint arXiv:2007.01434</em>.</li>
            <li>Heinze-Deml, C., Peters, J., & Meinshausen, N. (2018). Invariant causal prediction for nonlinear models. <em>Journal of Causal Inference</em>, 6(2), 20170016.</li>
            <li>Janzing, D., & Schölkopf, B. (2015). Semi-supervised interpolation in an anticausal learning scenario. <em>The Journal of Machine Learning Research</em>, 16(1), 1923–1948.</li>
            <li>Jiang, Y., & Veitch, V. (2022). Invariant and transportable representations for anti-causal domain shifts. <em>Advances in Neural Information Processing Systems</em>, 35, 20782–20794.</li>
            <li>Kim, B., Kim, H., Kim, K., Kim, S., & Kim, J. (2019). Learning not to learn: Training deep neural networks with biased data. <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 9012–9020.</li>
            <li>Klenke, A. (2014). <em>Probability Theory: A Comprehensive Course</em> (2nd ed.). Springer.</li>
            <li>Kocaoglu, M., Jaber, A., Shanmugam, K., & Bareinboim, E. (2019). Characterization and learning of causal graphs with latent variables from soft interventions. <em>Advances in Neural Information Processing Systems</em>, 32.</li>
            <li>Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D., Le Priol, R., & Courville, A. (2021). Out-of-distribution generalization via risk extrapolation (rex). <em>International Conference on Machine Learning</em>.</li>
            <li>Lemma, A. (2020). Measure Theory Details. <em>Mathematical Notes</em>.</li>
            <li>Li, H., Pan, S. J., Wang, S., & Kot, A. C. (2018). Domain generalization with adversarial feature learning. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 5400–5409.</li>
            <li>Liu, C., Sun, X., Wang, J., Tang, H., Li, T., Qin, T., Chen, W., & Liu, T.-Y. (2021). Learning causal semantic representation for out-of-distribution prediction. <em>Advances in Neural Information Processing Systems</em>, 34, 6155–6170.</li>
            <li>Long, M., Cao, Z., Wang, J., & Jordan, M. I. (2018). Conditional adversarial domain adaptation. <em>Advances in neural information processing systems</em>, 31.</li>
            <li>Lu, C., Wu, Y., Hernández-Lobato, J. M., & Schölkopf, B. (2021). Invariant causal representation learning for out-of-distribution generalization. <em>International Conference on Learning Representations</em>.</li>
            <li>Lv, F., Liang, J., Li, S., Zang, B., Liu, C. H., Wang, Z., & Liu, D. (2022). Causality inspired representation learning for domain generalization. <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>.</li>
            <li>Makar, M., & D’Amour, A., (2022). Fairness and robustness in anti-causal prediction. <em>arXiv preprint arXiv:2209.09423</em>.</li>
            <li>Mejia, S. H. G., Blöbaum, P., Schölkopf, B., & Janzing, D. (2025). Causal vs. Anticausal merging of predictors. <em>arXiv preprint arXiv:2501.08426</em>.</li>
            <li>Mitrovic, J., Sejdinovic, D., & Teh, Y. W. (2018). Causal inference via kernel deviance measures. <em>Advances in neural information processing systems</em>, 31.</li>
            <li>Nam, H., Lee, H., Park, J., Yoon, W., & Yoo, D. (2021). Reducing domain gap by reducing style bias. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 8690–8699.</li>
            <li>Noorbakhsh, S. L., Zhang, B., Hong, Y., & Wang, B. (2024). $\{$Inf2Guard$\}$: An $\{$Information-Theoretic$\}$ Framework for Learning $\{$Privacy-Preserving$\}$ Representations against Inference Attacks. <em>USENIX Security Symposium</em>.</li>
            <li>Park, J., Buchholz, S., Schölkopf, B., & Muandet, K. (2023). A Measure-Theoretic Axiomatisation of Causality. <em>Advances in Neural Information Processing Systems</em>, 36, 28510–28540.</li>
            <li>Pearl, J. (2009). <em>Causality</em>. Cambridge university press.</li>
            <li>Peters, J., Bühlmann, P., & Meinshausen, N. (2016). Causal inference by using invariant prediction: identification and confidence intervals. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, 78(5), 947–1012.</li>
            <li>Pezeshki, M., Bouchacourt, D., Ibrahim, M., Ballas, N., Vincent, P., & Lopez-Paz, D. (2023). Discovering environments with XRM. <em>arXiv preprint arXiv:2309.16748</em>.</li>
            <li>Ramé, A., Ahuja, K., Zhang, J., Cord, M., Bottou, L., & Lopez-Paz, D. (2023). Model ratatouille: Recycling diverse models for out-of-distribution generalization. <em>International Conference on Machine Learning</em>, 28656–28679.</li>
            <li>Sagawa, S., Koh, P. W., Hashimoto, T. B., & Liang, P. (2019). Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. <em>arXiv preprint arXiv:1911.08731</em>.</li>
            <li>Scherrer, N., Bilaniuk, O., Annadani, Y., Goyal, A., Schwab, P., Schölkopf, B., Mozer, M. C., Bengio, Y., Bauer, S., & Ke, N. R. (2021). Learning neural causal models with active interventions. <em>arXiv preprint arXiv:2109.02429</em>.</li>
            <li>Schölkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., & Mooij, J. (2012). On causal and anticausal learning. <em>arXiv preprint arXiv:1206.6471</em>.</li>
            <li>Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., & Bengio, Y. (2021). Toward causal representation learning. <em>Proceedings of the IEEE</em>, 109(5), 612–634.</li>
            <li>Shi, J., Gare, G., Tian, J., Chai, S., Lin, Z., Vasudevan, A., Feng, D., Ferroni, F., & Kong, S. (2024). LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies. <em>arXiv preprint arXiv:2407.16067</em>.</li>
            <li>Squires, C., Seigal, A., Bhate, S. S., & Uhler, C. (2023). Linear Causal Disentanglement via Interventions. <em>International Conference on Machine Learning</em>.</li>
            <li>Squires, C., & Uhler, C. (2023). Causal structure learning: A combinatorial perspective. <em>Foundations of Computational Mathematics</em>, 23(5), 1781–1815.</li>
            <li>Sriperumbudur, B. K., Fukumizu, K., Gretton, A., Schölkopf, B., & Lanckriet, G. R. G. (2009). On integral probability metrics, $\phi$-divergences and binary classification. <em>arXiv preprint arXiv:0901.2698</em>.</li>
            <li>Sui, Y., Wu, Q., Wu, J., Cui, Q., Li, L., Zhou, J., Wang, X., & He, X. (2024). Unleashing the power of graph data augmentation on covariate distribution shift. <em>Advances in Neural Information Processing Systems</em>, 36.</li>
            <li>Sun, B., & Saenko, K. (2016). Deep coral: Correlation alignment for deep domain adaptation. <em>Computer vision—ECCV 2016 workshops</em>.</li>
            <li>Tachet des Combes, R., Zhao, H., Wang, Y.-X., & Gordon, G. J. (2020). Domain adaptation with conditional distribution matching and generalized label shift. <em>Advances in Neural Information Processing Systems</em>, 33, 19276–19289.</li>
            <li>Tellez, D., Litjens, G., Bándi, P., Bulten, W., Bokhorst, J.-M., Ciompi, F., & Van Der Laak, J. (2019). Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. <em>Medical image analysis</em>, 58, 101544.</li>
            <li>Veitch, V., D'Amour, A., Yadlowsky, S., & Eisenstein, J. (2021). Counterfactual invariance to spurious correlations: Why and how to pass stress tests. <em>arXiv preprint arXiv:2106.00545</em>.</li>
            <li>Vesentini, E. (1979). Variations on a theme of Carathéodory. <em>Annali della Scuola Normale Superiore di Pisa-Classe di Scienze</em>, 6(1), 39–68.</li>
            <li>Vedantam, R., Lopez-Paz, D., & Schwab, D. J. (2021). An Empirical Investigation of Domain Generalization with Empirical Risk Minimizers. <em>Advances in Neural Information Processing Systems</em>.</li>
            <li>von Kügelgen, J., Besserve, M., Wendong, L., Gresele, L., Kekiċ, A., Bareinboim, E., Blei, D., & Schölkopf, B. (2023). Nonparametric Identifiability of Causal Representations from Unknown Interventions. <em>Advances in Neural Information Processing Systems</em>.</li>
            <li>Wang, B., Guo, J., Li, A., Chen, Y., & Li, H. (2021). Privacy-preserving representation learning on graphs: A mutual information perspective. <em>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>.</li>
            <li>Wang, X., Chen, H., Wu, Z., & Zhu, W. (2024). Disentangled representation learning. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</li>
            <li>Wang, Z., & Veitch, V. (2022). A unified causal view of domain invariant representation learning. <em>arXiv preprint arXiv:2208.06987</em>.</li>
            <li>Wang, Z., & Veitch, V. (2022). The Causal Structure of Domain Invariant Supervised Representation Learning. <em>arXiv preprint arXiv:2208.06987</em>.</li>
            <li>Wendong, L., Kekiċ, A., von Kügelgen, J., Buchholz, S., Besserve, M., Gresele, L., & Schölkopf, B. (2023). Causal component analysis. <em>Advances in Neural Information Processing Systems</em>, 36, 32481–32520.</li>
            <li>Wendong, L., Kekiċ, A., von Kügelgen, J., Buchholz, S., Besserve, M., Gresele, L., & Schölkopf, B. (2024). Causal component analysis. <em>Advances in Neural Information Processing Systems</em>, 36.</li>
            <li>Worrall, D. E., Garbin, S. J., Turmukhambetov, D., & Brostow, G. J. (2017). Harmonic networks: Deep translation and rotation equivariance. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 5028–5037.</li>
            <li>Xia, Y., Yang, P., Xu, Z., Li, Y., & Xue, Y. (2024). Neural Causal Representation Learning for High-dimensional Time Series. <em>International Conference on Learning Representations</em>.</li>
            <li>Yao, W., Chen, G., & Zhang, K. (2022). Learning latent causal dynamics. <em>arXiv preprint arXiv:2202.04828</em>.</li>
            <li>Ye, H., Xie, C., Cai, T., Li, R., Li, Z., & Wang, L. (2021). Towards a theoretical framework of out-of-distribution generalization. <em>Advances in Neural Information Processing Systems</em>, 34, 23519–23531.</li>
            <li>Yi, M., Hou, L., Sun, J., Shang, L., Jiang, X., Liu, Q., & Ma, Z. (2021). Improved ood generalization via adversarial training and pretraing. <em>International Conference on Machine Learning</em>, 11987–11997.</li>
            <li>Yu, H., Zhang, X., Xu, R., Liu, J., He, Y., & Cui, P. (2023). Rethinking the Evaluation Protocol of Domain Generalization. <em>arXiv preprint arXiv:2305.15253</em>.</li>
            <li>Zhang, B., Noorbakhsh, S. L., Dong, Y., Hong, Y., & Wang, B. (2025). Learning Robust and Privacy-Preserving Representations via Information Theory. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 39, 22363–22371.</li>
            <li>Zhang, X., Cui, P., Xu, R., Zhou, L., He, Y., & Shen, Z. (2021). Deep stable learning for out-of-distribution generalization. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 5372–5382.</li>
            <li>Zhou, D., Wang, N., Gao, X., Han, B., Wang, X., Zhan, Y., & Liu, T. (2022). Improving adversarial robustness via mutual information estimation. <em>International conference on machine learning</em>.</li>
            <li>Zhou, K., Yang, Y., Hospedales, T., & Xiang, T. (2020). Deep domain-adversarial image generation for domain generalisation. <em>Proceedings of the AAAI conference on artificial intelligence</em>, 34, 13025–13032.</li>
            <li>Zhou, K., Yang, Y., Hospedales, T., & Xiang, T. (2020). Learning to generate novel domains for domain generalization. <em>ECCV</em>, 561–578.</li>
            <li>Zhu, S., Zhang, X., & Evans, D. (2020). Learning adversarially robust representations via worst-case mutual information maximization. <em>International Conference on Machine Learning</em>.</li>
        </ol>

    </div>
</body>
</html>